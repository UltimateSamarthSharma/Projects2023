{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"0\"></a>\n",
    "\n",
    "# <p style=\"padding:10px;background-color:#87CEEB ;margin:10;color:#000000;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Assignment  41: Feature Engineering 1 - Utkarsh Gaikwad</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Assignment pdf link](17%20Mar_AssQ.pdf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "\n",
    "# <p style=\"padding:10px;background-color:#87CEEB ;margin:10;color:#000000;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Question 1</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 : What are missing values in a dataset? Why is it essential to handle missing values? Name some algorithms that are not affected by missing values.\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer :"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing values in a dataset refer to the absence of data in one or more fields or attributes. They can occur due to various reasons, such as incomplete data collection, data entry errors, or data corruption.\n",
    "\n",
    "### It is essential to handle missing values in a dataset because they can negatively impact the quality and accuracy of data analysis and machine learning models. Missing values can cause bias in statistical analysis and prediction models, leading to incorrect conclusions and inaccurate predictions. Additionally, most machine learning algorithms cannot handle missing values, and they may either fail to produce results or produce suboptimal results.\n",
    "\n",
    "### Some algorithms that are not affected by missing values include decision trees, random forests, and gradient boosting algorithms such as XGBoost and LightGBM. These algorithms are capable of handling missing values by using a variety of techniques such as surrogate splits, imputation, or treating missing values as a separate category. However, it is always recommended to handle missing values appropriately to avoid introducing errors and biases in data analysis and modeling."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Missing Values in a data](https://149695847.v2.pressablecdn.com/wp-content/uploads/2018/02/missing-values.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>\n",
    "\n",
    "# <p style=\"padding:10px;background-color:#87CEEB ;margin:10;color:#000000;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Question 2</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2 : List down techniques used to handle missing data. Give an example of each with python code.\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer :"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here are some common techniques used to handle missing data in a dataset, along with examples in Python code:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Deletion: \n",
    "This involves removing the rows or columns that contain missing values from the dataset. This technique is appropriate when the missing values are random and the amount of data loss is acceptable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data before deletion of missing rows : \n",
      "     A     B     C\n",
      "0  1.0   6.0  11.0\n",
      "1  2.0   NaN  12.0\n",
      "2  3.0   8.0   NaN\n",
      "3  NaN   9.0  14.0\n",
      "4  5.0  10.0  15.0\n",
      "\n",
      "====================================\n",
      "\n",
      "Data after deletion of missing rows : \n",
      "     A     B     C\n",
      "0  1.0   6.0  11.0\n",
      "4  5.0  10.0  15.0\n"
     ]
    }
   ],
   "source": [
    "# Example of deleting rows with missing values\n",
    "import pandas as pd\n",
    "\n",
    "# create a sample dataset with missing values\n",
    "data = {'A': [1, 2, 3, None, 5],\n",
    "        'B': [6, None, 8, 9, 10],\n",
    "        'C': [11, 12, None, 14, 15]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Print dataset before deletion\n",
    "print('Data before deletion of missing rows : ')\n",
    "print(df)\n",
    "\n",
    "print('\\n====================================\\n')\n",
    "\n",
    "# drop rows with missing values\n",
    "df = df.dropna()\n",
    "\n",
    "# Print dataset after deletion\n",
    "print('Data after deletion of missing rows : ')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data before deletion of missing columns : \n",
      "   A   B     C\n",
      "0  1   6  11.0\n",
      "1  2   7  12.0\n",
      "2  3   8   NaN\n",
      "3  4   9   NaN\n",
      "4  5  10  15.0\n",
      "\n",
      "====================================\n",
      "\n",
      "Data after deletion of missing columns : \n",
      "   A   B\n",
      "0  1   6\n",
      "1  2   7\n",
      "2  3   8\n",
      "3  4   9\n",
      "4  5  10\n"
     ]
    }
   ],
   "source": [
    "# Example of deleting columns with missing values\n",
    "import pandas as pd\n",
    "\n",
    "# create a sample dataset with missing values\n",
    "data = {'A': [1, 2, 3, 4, 5],\n",
    "        'B': [6, 7 , 8, 9, 10],\n",
    "        'C': [11, 12, None, None, 15]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Print dataset before deletion\n",
    "print('Data before deletion of missing columns : ')\n",
    "print(df)\n",
    "\n",
    "print('\\n====================================\\n')\n",
    "\n",
    "# drop rows with missing values\n",
    "df = df.dropna(axis=1)\n",
    "\n",
    "# Print dataset after deletion\n",
    "print('Data after deletion of missing columns : ')\n",
    "print(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Simple Imputation: \n",
    "This involves filling in the missing values with an estimated value based on the available data. This technique is appropriate when the missing values are non-random and the amount of missing data is relatively small."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Mean Imputation : This should be used on numerical variables when there are no outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data before mean imputation : \n",
      "     A     B     C\n",
      "0  1.0   6.0  11.0\n",
      "1  2.0   NaN  12.0\n",
      "2  3.0   8.0   NaN\n",
      "3  NaN   9.0  14.0\n",
      "4  5.0  10.0  15.0\n",
      "\n",
      "====================================\n",
      "\n",
      "Data after mean imputation : \n",
      "      A      B     C\n",
      "0  1.00   6.00  11.0\n",
      "1  2.00   8.25  12.0\n",
      "2  3.00   8.00  13.0\n",
      "3  2.75   9.00  14.0\n",
      "4  5.00  10.00  15.0\n"
     ]
    }
   ],
   "source": [
    "# Example of imputing missing values with mean value\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# create a sample dataset with missing values\n",
    "data = {'A': [1, 2, 3, None, 5],\n",
    "        'B': [6, None, 8, 9, 10],\n",
    "        'C': [11, 12, None, 14, 15]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Print dataset before imputation\n",
    "print('Data before mean imputation : ')\n",
    "print(df)\n",
    "\n",
    "print('\\n====================================\\n')\n",
    "\n",
    "# impute missing values with mean value\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "df_imputed = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)\n",
    "\n",
    "print('Data after mean imputation : ')\n",
    "print(df_imputed)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Median Imputation : This should be used on numerical variables when there are outliers present in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data before median imputation : \n",
      "      A      B      C\n",
      "0   1.0    6.0   11.0\n",
      "1   2.0    NaN   12.0\n",
      "2   3.0    8.0    NaN\n",
      "3   NaN    9.0   14.0\n",
      "4   5.0   10.0   15.0\n",
      "5  90.0  200.0  100.0\n",
      "\n",
      "====================================\n",
      "\n",
      "Data after median imputation : \n",
      "      A      B      C\n",
      "0   1.0    6.0   11.0\n",
      "1   2.0    9.0   12.0\n",
      "2   3.0    8.0   14.0\n",
      "3   3.0    9.0   14.0\n",
      "4   5.0   10.0   15.0\n",
      "5  90.0  200.0  100.0\n"
     ]
    }
   ],
   "source": [
    "# Example of imputing missing values with median value\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# create a sample dataset with missing values\n",
    "data = {'A': [1, 2, 3, None, 5, 90],\n",
    "        'B': [6, None, 8, 9, 10, 200],\n",
    "        'C': [11, 12, None, 14, 15, 100]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Print dataset before imputation\n",
    "print('Data before median imputation : ')\n",
    "print(df)\n",
    "\n",
    "print('\\n====================================\\n')\n",
    "\n",
    "# impute missing values with mean value\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "df_imputed = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)\n",
    "print('Data after median imputation : ')\n",
    "print(df_imputed)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Mode Imputation : This should be used to handle categorical misssing data only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data before mode imputation on categorical data : \n",
      "  Gender City   Age   Income\n",
      "0      M  NYC  30.0  50000.0\n",
      "1      F   LA  40.0      NaN\n",
      "2      F  NaN  25.0  80000.0\n",
      "3    NaN   LA   NaN  60000.0\n",
      "4      M   LA   NaN  70000.0\n",
      "5      F  NYC  35.0      NaN\n",
      "6      M  NaN  28.0  90000.0\n",
      "\n",
      "====================================\n",
      "\n",
      "Data after mode imputation on categorical data : \n",
      "  Gender City   Age   Income\n",
      "0      M  NYC  30.0  50000.0\n",
      "1      F   LA  40.0      NaN\n",
      "2      F   LA  25.0  80000.0\n",
      "3      F   LA   NaN  60000.0\n",
      "4      M   LA   NaN  70000.0\n",
      "5      F  NYC  35.0      NaN\n",
      "6      M   LA  28.0  90000.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Create a sample dataframe with missing values\n",
    "data = {'Gender': ['M', 'F', 'F', np.nan, 'M', 'F', 'M'],\n",
    "        'City': ['NYC', 'LA', np.nan, 'LA', 'LA', 'NYC', np.nan],\n",
    "        'Age': [30, 40, 25, np.nan, np.nan, 35, 28],\n",
    "        'Income': [50000, np.nan, 80000, 60000, 70000, np.nan, 90000]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Print dataset before imputation\n",
    "print('Data before mode imputation on categorical data : ')\n",
    "print(df)\n",
    "\n",
    "print('\\n====================================\\n')\n",
    "\n",
    "\n",
    "# Create a SimpleImputer object with 'most_frequent' strategy\n",
    "imputer = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "# Impute the missing values in the categorical columns\n",
    "df[['Gender', 'City']] = imputer.fit_transform(df[['Gender', 'City']])\n",
    "\n",
    "# Display the dataframe after imputation\n",
    "print('Data after mode imputation on categorical data : ')\n",
    "print(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. K-nearest neighbor imputation: \n",
    "This involves filling in the missing values with the values of the nearest neighbor(s) based on a distance metric. This technique is appropriate when the missing values are non-random, and there is a correlation between the missing values and the other features. Should be used on numerical variables only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data before KNN imputation : \n",
      "     A     B     C\n",
      "0  1.0   6.0   NaN\n",
      "1  2.0   NaN  12.0\n",
      "2  NaN   NaN  13.0\n",
      "3  NaN   9.0  14.0\n",
      "4  5.0  10.0   NaN\n",
      "\n",
      "====================================\n",
      "\n",
      "Data after KNN imputation : \n",
      "          A          B     C\n",
      "0  1.000000   6.000000  13.0\n",
      "1  2.000000   8.333333  12.0\n",
      "2  2.666667   8.333333  13.0\n",
      "3  2.666667   9.000000  14.0\n",
      "4  5.000000  10.000000  13.0\n"
     ]
    }
   ],
   "source": [
    "# Example of k-nearest neighbor imputation\n",
    "import pandas as pd\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# create a sample dataset with missing values\n",
    "data = {'A': [1, 2, None, None, 5],\n",
    "        'B': [6, None, None, 9, 10],\n",
    "        'C': [None, 12, 13, 14, None]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Print dataset before imputation\n",
    "print('Data before KNN imputation : ')\n",
    "print(df)\n",
    "\n",
    "print('\\n====================================\\n')\n",
    "\n",
    "# impute missing values with k-nearest neighbor imputation\n",
    "imputer = KNNImputer(n_neighbors=3)\n",
    "df_imputed = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)\n",
    "\n",
    "print('Data after KNN imputation : ')\n",
    "print(df_imputed)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Iterative Imputer:\n",
    "It is an advanced imputation technique used to handle missing data in a dataset. This technique uses machine learning algorithms to estimate the missing values based on the observed data. It iteratively imputes missing values by modeling each feature with missing values as a function of other features in the dataset that do not have missing values. Used on Numerical data only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data before Iterative imputation : \n",
      "     A    B     C\n",
      "0  1.0  6.0  11.0\n",
      "1  2.0  NaN  12.0\n",
      "2  3.0  8.0   NaN\n",
      "3  NaN  9.0  14.0\n",
      "4  5.0  NaN  15.0\n",
      "\n",
      "====================================\n",
      "\n",
      "Data after Iterative imputation : \n",
      "     A         B     C\n",
      "0  1.0  6.000000  11.0\n",
      "1  2.0  7.000000  12.0\n",
      "2  3.0  8.000000  13.0\n",
      "3  4.0  9.000000  14.0\n",
      "4  5.0  9.999999  15.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "# create a sample dataset with missing values\n",
    "data = {'A': [1, 2, 3, None, 5],\n",
    "        'B': [6, None, 8, 9, None],\n",
    "        'C': [11, 12, None, 14, 15]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# print Data before iterative imputation\n",
    "print('Data before Iterative imputation : ')\n",
    "print(df)\n",
    "\n",
    "print('\\n====================================\\n')\n",
    "\n",
    "\n",
    "# impute missing values with iterative imputer\n",
    "imputer = IterativeImputer(max_iter=10, random_state=42)\n",
    "df_imputed = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)\n",
    "\n",
    "print('Data after Iterative imputation : ')\n",
    "print(df_imputed)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a>\n",
    "\n",
    "# <p style=\"padding:10px;background-color:#87CEEB ;margin:10;color:#000000;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Question 3</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3 : Explain the imbalanced data. What will happen if imbalanced data is not handled?\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer :"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imbalanced data refers to a situation where the classes or categories in a dataset are not equally represented. This means that one or more classes have significantly fewer samples than the others. Imbalanced data is common in many real-world applications such as fraud detection, medical diagnosis, and rare event prediction.\n",
    "\n",
    "### The problem with imbalanced data is that most machine learning algorithms are designed to assume that the classes are balanced, and they tend to perform poorly when applied to imbalanced data. This is because the algorithms tend to be biased towards the majority class, which can lead to poor performance on the minority class. For example, if a dataset contains 95% samples of Class A and only 5% samples of Class B, a classifier trained on this dataset is likely to predict most new examples as Class A, regardless of their actual class.\n",
    "\n",
    "### If imbalanced data is not handled, it can lead to several problems, including:\n",
    "\n",
    "1. Poor performance: The performance of a classifier trained on imbalanced data is likely to be poor, particularly on the minority class. This can lead to false negatives and false positives, which can have serious consequences in some applications.\n",
    "\n",
    "2. Biased models: Imbalanced data can lead to biased models that are not representative of the true distribution of the data. This can result in poor generalization to new examples and can make the model less reliable.\n",
    "\n",
    "3. Overfitting: In imbalanced datasets, the model can learn to overfit on the majority class, which can lead to poor performance on the minority class.\n",
    "\n",
    "### To handle imbalanced data, several techniques can be used, including:\n",
    "\n",
    "1. Resampling: This involves either oversampling the minority class or undersampling the majority class to create a balanced dataset.\n",
    "\n",
    "2. Cost-sensitive learning: This involves assigning different misclassification costs to different classes to reflect the imbalance in the data.\n",
    "\n",
    "3. Algorithmic modifications: This involves modifying the machine learning algorithm to handle imbalanced data directly, such as changing the threshold of a decision rule or using specialized classifiers designed for imbalanced data.\n",
    "\n",
    "### By handling imbalanced data, we can improve the performance and reliability of machine learning models and ensure that they are more representative of the true distribution of the data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Balanced vs imbalanced data](https://miro.medium.com/v2/resize:fit:450/1*zsyN08VVrgHbAEdvv27Pyw.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4\"></a>\n",
    "\n",
    "# <p style=\"padding:10px;background-color:#87CEEB ;margin:10;color:#000000;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Question 4</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4 : What are Up-sampling and Down-sampling? Explain with an example when up-sampling and downsampling are required\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer :"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upsampling and downsampling are two common techniques used to handle imbalanced data in machine learning.\n",
    "\n",
    "### Downsampling involves reducing the number of samples in the majority class to match the number of samples in the minority class. This can be done randomly or using more sophisticated techniques, such as clustering or instance selection. Downsampling is useful when the majority class has a large number of samples that can be safely removed without losing important information.\n",
    "\n",
    "### For example, consider a dataset with 1000 samples of Class A and 100 samples of Class B. If we downsample Class A to 100 samples, we can create a balanced dataset with 100 samples of each class.\n",
    "\n",
    "### Upsampling, on the other hand, involves increasing the number of samples in the minority class to match the number of samples in the majority class. This can be done by replicating existing samples in the minority class, or by generating new synthetic samples using techniques such as SMOTE (Synthetic Minority Over-sampling Technique). Upsampling is useful when the minority class has a small number of samples that cannot be safely removed, and when we want to avoid losing important information.\n",
    "\n",
    "### For example, consider a dataset with 1000 samples of Class A and 100 samples of Class B. If we upsample Class B to 1000 samples using SMOTE, we can create a balanced dataset with 1000 samples of each class.\n",
    "\n",
    "### Whether to use upsampling or downsampling depends on the specific dataset and problem at hand. In general, upsampling is preferred when the minority class is important and has important features that need to be preserved, while downsampling is preferred when the majority class is too large to process efficiently or contains a significant amount of irrelevant data.\n",
    "\n",
    "### In summary, upsampling and downsampling are two techniques used to handle imbalanced data in machine learning. Upsampling involves increasing the number of samples in the minority class, while downsampling involves reducing the number of samples in the majority class. The choice of which technique to use depends on the specific dataset and problem at hand."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Undersampling vs Oversampling](https://miro.medium.com/v2/resize:fit:725/0*FeIp1t4uEcW5LmSM.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5\"></a>\n",
    "\n",
    "# <p style=\"padding:10px;background-color:#87CEEB ;margin:10;color:#000000;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Question 5</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5 : What is data Augmentation? Explain SMOTE.\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer :"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data augmentation is a technique used to increase the size and diversity of a dataset by creating new synthetic examples based on the existing data. This technique is commonly used in machine learning to improve model performance, particularly in situations where the available dataset is small or imbalanced.\n",
    "\n",
    "### One popular data augmentation technique is SMOTE (Synthetic Minority Over-sampling Technique). SMOTE is specifically designed to handle imbalanced datasets where the minority class has very few samples. SMOTE generates synthetic examples of the minority class by interpolating between pairs of minority class examples.\n",
    "\n",
    "### The basic idea of SMOTE is to randomly select a minority class example and its k nearest neighbors, where k is a user-defined parameter. SMOTE then creates new synthetic examples by interpolating between the minority example and each of its k nearest neighbors. Specifically, SMOTE selects a random point along the line segment connecting the minority example and its nearest neighbor and adds this point as a new example to the dataset.\n",
    "\n",
    "### This process is repeated until the desired number of synthetic examples has been generated. The result is a larger and more diverse dataset that includes synthetic examples of the minority class.\n",
    "\n",
    "### SMOTE can be very effective in improving the performance of machine learning models on imbalanced datasets. By creating synthetic examples of the minority class, SMOTE can help to address the problem of class imbalance and ensure that the model is better able to generalize to new examples.\n",
    "\n",
    "### However, it is important to note that SMOTE can also introduce some noise and overfitting in the data, particularly if the value of k is set too high. Therefore, it is important to carefully select the parameters of SMOTE and to evaluate its effectiveness using appropriate validation techniques."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![SMOTE](https://iq.opengenus.org/content/images/2019/09/COVER-1.PNG)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"6\"></a>\n",
    "\n",
    "# <p style=\"padding:10px;background-color:#87CEEB ;margin:10;color:#000000;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Question 6</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6 : What are outliers in a dataset? Why is it essential to handle outliers?\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer :"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers are data points that are significantly different from other data points in a dataset. These data points can be either very high or very low in value, and they can have a significant impact on statistical analysis and machine learning models.\n",
    "\n",
    "### It is essential to handle outliers because they can cause a number of problems, including:\n",
    "\n",
    "1. Skewed data distribution: Outliers can distort the data distribution, making it difficult to accurately interpret the data and identify patterns.\n",
    "\n",
    "2. Misleading statistical measures: Outliers can significantly affect statistical measures such as mean and standard deviation, leading to inaccurate or misleading results.\n",
    "\n",
    "3. Biased machine learning models: Outliers can have a disproportionate influence on the model training process, leading to biased models that perform poorly on new data.\n",
    "\n",
    "4. Reduced model performance: Outliers can cause overfitting, leading to reduced model performance and accuracy.\n",
    "\n",
    "### There are several techniques that can be used to handle outliers in a dataset. Some of these techniques include:\n",
    "\n",
    "1. Visual inspection: One of the simplest ways to identify outliers is by visually inspecting the data using box plots, scatter plots, and other visualization techniques.\n",
    "\n",
    "2. Statistical methods: Statistical methods such as Z-score, interquartile range (IQR), and Tukey's method can be used to identify outliers based on their distance from the mean or median of the data.\n",
    "\n",
    "3. Machine learning algorithms: Some machine learning algorithms, such as isolation forest and local outlier factor, are specifically designed to identify outliers in a dataset.\n",
    "\n",
    "4. Data transformation: Data transformation techniques such as normalization and logarithmic scaling can be used to reduce the impact of outliers on statistical analysis and machine learning models.\n",
    "\n",
    "### Handling outliers is essential for accurate and reliable data analysis and machine learning. By identifying and removing or reducing the impact of outliers, we can improve the quality and accuracy of our results, leading to better insights and more effective decision making."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Outliers](https://cdn.scribbr.com/wp-content/uploads/2022/01/interquartile-range.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"7\"></a>\n",
    "\n",
    "# <p style=\"padding:10px;background-color:#87CEEB ;margin:10;color:#000000;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Question 7</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7 : You are working on a project that requires analyzing customer data. However, you notice that some of the data is missing. What are some techniques you can use to handle the missing data in your analysis?\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer :"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There are several techniques that can be used to handle missing data in customer data analysis:\n",
    "\n",
    "1. Deletion: One simple approach is to simply delete any rows or columns with missing data. However, this approach can lead to loss of important information and reduce the size of the dataset.\n",
    "\n",
    "2. Imputation: Imputation involves replacing missing data with estimated values based on the available data. This can be done using techniques such as mean imputation, median imputation, mode imputation, and iterative imputation.\n",
    "\n",
    "3. Regression: Regression analysis can be used to predict missing values based on the available data. This approach can be particularly effective if there is a strong correlation between the missing variable and other variables in the dataset.\n",
    "\n",
    "4. Multiple imputation: Multiple imputation involves creating multiple imputed datasets and combining them to produce a final estimate of the missing values. This approach can be particularly effective if there is a significant amount of missing data in the dataset.\n",
    "\n",
    "5. Machine learning: Machine learning algorithms can be used to predict missing values based on the available data. This approach can be particularly effective if the dataset contains complex relationships between variables.\n",
    "\n",
    "### The choice of technique will depend on the nature of the missing data, the size of the dataset, and the specific requirements of the analysis. It is important to carefully evaluate the effectiveness of each technique and to consider the potential impact of missing data on the analysis results."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"8\"></a>\n",
    "\n",
    "# <p style=\"padding:10px;background-color:#87CEEB ;margin:10;color:#000000;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Question 8</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 8 : You are working with a large dataset and find that a small percentage of the data is missing. What are some strategies you can use to determine if the missing data is missing at random or if there is a pattern to the missing data?\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer :"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When dealing with missing data, there are several strategies to determine if the missing data is missing at random or if there is a pattern to the missing data. Here are some of the most commonly used methods:\n",
    "\n",
    "1. Analyze missingness patterns: You can start by examining the missingness patterns in the data. Plotting the distribution of missing values by variable or by record can help identify patterns of missingness. If the missingness patterns are random or similar across all variables, then it is likely that the missing data is missing at random. However, if there are patterns in the missingness, such as specific variables having higher rates of missing values or specific values within a variable being more likely to be missing, this suggests that the missing data may be non-random.\n",
    "\n",
    "2. Correlation analysis: You can examine the correlation between the missingness of a variable and other variables in the dataset. If the missingness of a variable is not correlated with any other variable, then it is likely missing at random. However, if the missingness of a variable is correlated with other variables, it suggests that the missing data may be non-random.\n",
    "\n",
    "3. Imputation and analysis: Impute the missing values using various techniques and compare the results. If the results are consistent across multiple imputation techniques, then it suggests that the missing data is missing at random. However, if the results vary significantly depending on the imputation technique used, it suggests that the missing data may be non-random.\n",
    "\n",
    "4. Expert knowledge: Sometimes expert knowledge can help determine if the missing data is missing at random or not. For example, if you are studying the impact of a new medication, and patients who experience side effects are more likely to drop out of the study, then the missing data is likely not missing at random.\n",
    "\n",
    "5. Statistical tests: You can use statistical tests such as the Little’s MCAR test or Missing Completely at Random (MCAR) test to determine if the missing data is missing at random or not. These tests can help determine if the pattern of missing data can be explained by chance or if there is a systematic reason for the missing data.\n",
    "\n",
    "### Overall, it's important to remember that determining the pattern of missing data is often a combination of these methods, and it may require some judgment to make a final determination."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"9\"></a>\n",
    "\n",
    "# <p style=\"padding:10px;background-color:#87CEEB ;margin:10;color:#000000;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Question 9</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 9 : Suppose you are working on a medical diagnosis project and find that the majority of patients in the dataset do not have the condition of interest, while a small percentage do. What are some strategies you can use to evaluate the performance of your machine learning model on this imbalanced dataset?\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer :"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with imbalanced datasets is a common problem in machine learning, especially in medical diagnosis projects. Here are some strategies you can use to evaluate the performance of your machine learning model on an imbalanced dataset:\n",
    "\n",
    "1. Confusion matrix: A confusion matrix is a table that summarizes the performance of a classification model. It shows the true positive, false positive, true negative, and false negative rates. In the case of an imbalanced dataset, accuracy may not be a good metric to evaluate the model's performance. Instead, you can look at other metrics such as precision, recall, F1-score, and the area under the receiver operating characteristic (ROC) curve. These metrics are not affected by the class imbalance and provide a better evaluation of the model's performance.\n",
    "\n",
    "2. Resampling techniques: Resampling techniques can be used to balance the dataset. You can either oversample the minority class or undersample the majority class. Oversampling involves adding copies of the minority class to the dataset, while undersampling involves removing examples from the majority class. However, both techniques have some drawbacks. Oversampling can lead to overfitting, while undersampling can lead to a loss of information. One common resampling technique is SMOTE (Synthetic Minority Over-sampling Technique), which generates synthetic examples of the minority class.\n",
    "\n",
    "3. Ensemble methods: Ensemble methods combine multiple models to improve their performance. One common ensemble method is the bagging method, which involves training multiple models on different subsets of the dataset and averaging their predictions. Another common ensemble method is the boosting method, which involves training multiple models sequentially, with each subsequent model focusing on the errors of the previous model. Ensemble methods can help improve the performance of the model on imbalanced datasets.\n",
    "\n",
    "4. Cost-sensitive learning: Cost-sensitive learning involves assigning different costs to different types of errors. In the case of an imbalanced dataset, misclassifying a minority class example as a majority class example may be more costly than the opposite. By assigning different costs to different types of errors, the model can be trained to minimize the overall cost of errors rather than just the number of errors.\n",
    "\n",
    "5. Domain knowledge: Finally, domain knowledge can be used to improve the model's performance on an imbalanced dataset. For example, if the dataset contains demographic information, you can use this information to stratify the dataset and ensure that both classes are represented equally in each stratum.\n",
    "\n",
    "### Overall, it's important to remember that there is no single best strategy for dealing with imbalanced datasets, and the best approach may depend on the specific dataset and problem at hand. It's often a combination of these techniques that leads to the best results."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"10\"></a>\n",
    "\n",
    "# <p style=\"padding:10px;background-color:#87CEEB ;margin:10;color:#000000;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Question 10</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 10 : When attempting to estimate customer satisfaction for a project, you discover that the dataset is unbalanced, with the bulk of customers reporting being satisfied. What methods can you employ to balance the dataset and down-sample the majority class?\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer :"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There are several methods that can be employed to balance an unbalanced dataset and down-sample the majority class. Here are a few possible approaches:\n",
    "\n",
    "1. Random under-sampling: This involves randomly removing instances from the majority class until the dataset is balanced. One potential drawback of this approach is that it may result in the loss of important information, particularly if the majority class contains important or rare examples that should be preserved.\n",
    "\n",
    "2. Cluster-based under-sampling: This method involves clustering the majority class instances and then selecting representative instances from each cluster. This can help to preserve important information in the majority class, while also reducing the imbalance.\n",
    "\n",
    "3. Tomek Links: This method is an under-sampling technique that identifies pairs of instances from different classes that are close to each other, and removes the majority class instance from each pair. By doing this, the Tomek Links method creates a clearer separation between the two classes.\n",
    "\n",
    "4. Edited Nearest Neighbors (ENN): This method is also an under-sampling technique that removes noisy or mislabeled instances by checking the class of each instance's nearest neighbors. If an instance's nearest neighbors are mostly from a different class, then the instance is removed. ENN can be applied after other under-sampling or over-sampling techniques to further improve the balance of the dataset.\n",
    "\n",
    "5. Ensemble-based methods: These methods involve training multiple models on different subsets of the data, and then combining the results to produce a final prediction. This can be particularly useful in cases where the dataset is highly imbalanced and standard methods may not be effective.\n",
    "\n",
    "### It is important to note that there is no one \"best\" method for balancing an unbalanced dataset, and the choice of method will depend on the specific characteristics of the dataset and the goals of the analysis. It is also important to evaluate the performance of the chosen method on a validation set to ensure that it does not introduce biases or negatively impact the accuracy of the model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"11\"></a>\n",
    "\n",
    "# <p style=\"padding:10px;background-color:#87CEEB ;margin:10;color:#000000;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Question 11</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 11 : You discover that the dataset is unbalanced with a low percentage of occurrences while working on a project that requires you to estimate the occurrence of a rare event. What methods can you employ to balance the dataset and up-sample the minority class?\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer :"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If I have an unbalanced dataset with a low percentage of occurrences of a rare event, you can employ various techniques to balance the dataset and up-sample the minority class. Here are a few possible approaches:\n",
    "\n",
    "1. Random over-sampling: This involves randomly duplicating instances from the minority class until the dataset is balanced. One potential drawback of this approach is that it may result in overfitting and lower the overall accuracy of the model.\n",
    "\n",
    "2. Synthetic minority over-sampling technique (SMOTE): This method involves creating synthetic instances of the minority class by interpolating between existing instances. SMOTE generates new instances by taking the difference between the feature vector of one minority class instance and its k-nearest neighbors, and then multiplying this difference by a random number between 0 and 1. This can help to balance the dataset while also preserving the overall distribution of the minority class.\n",
    "\n",
    "3. Adaptive Synthetic Sampling (ADASYN): This method is an extension of SMOTE that generates more synthetic instances in the minority class regions that are harder to learn by the classifier. The idea is to generate more synthetic samples where the density of the minority class is lower, thus focusing more on the difficult to learn samples.\n",
    "\n",
    "4. SMOTE-Tomek: This method combines the SMOTE over-sampling technique with Tomek Links under-sampling. Tomek Links are pairs of instances from different classes that are close to each other and can be removed to increase the separation between the classes. SMOTE-Tomek first applies applies SMOTE over-sampling to the remaining minority class instances., and then Tomek Links under-sampling to remove the majority class instances that form Tomek Links with minority class instances.\n",
    "\n",
    "5. SMOTE-ENN: This method combines the SMOTE over-sampling technique with Edited Nearest Neighbors (ENN) under-sampling. ENN is a cleaning technique that removes noisy or mislabeled instances by checking the class of each instance's nearest neighbors. SMOTE-ENN first applies SMOTE over-sampling to the minority class instances, and then applies ENN under-sampling to remove instances that are misclassified by their nearest neighbors.\n",
    "\n",
    "### It is important to note that up-sampling the minority class can also lead to overfitting and reduced generalization performance. Therefore, it is important to evaluate the performance of the chosen method on a validation set to ensure that it does not introduce biases or negatively impact the accuracy of the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
