{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebf55894-c724-4e7c-b832-157bd6570ee6",
   "metadata": {},
   "source": [
    "**Q1. What is the relationship between polynomial functions and kernel functions in machine learning algorithms?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878483f2-f5bb-4800-bef4-3f722ffb2d7f",
   "metadata": {},
   "source": [
    "Polynomial functions and kernel functions are both used in machine learning algorithms to transform data into a higher-dimensional space in order to make it easier to classify or analyze.\n",
    "\n",
    "Polynomial functions are a type of function that involve raising an input variable to various powers and multiplying by coefficients. Polynomial functions are commonly used in regression analysis, where they can be used to fit a curve to a set of data points. In machine learning, polynomial functions can be used as a basis function to transform data into a higher-dimensional space.\n",
    "\n",
    "Kernel functions, on the other hand, are a general class of functions that take two inputs and return a scalar value. In machine learning, kernel functions are used to compute the similarity between two data points in a feature space. The kernel function essentially measures the degree of similarity between two inputs, and this similarity metric is used to determine how close or far apart the inputs are in the transformed feature space.\n",
    "\n",
    "In many cases, polynomial functions can be used as kernel functions in machine learning algorithms. This is because polynomial functions can be expressed as a dot product of two vectors in a higher-dimensional space. Specifically, the polynomial kernel function is defined as:<br>**K(x, y) = (x^T y + c)^d**\n",
    "\n",
    "where x and y are input vectors, d is the degree of the polynomial, and c is a constant term. This kernel function computes the similarity between two inputs in a higher-dimensional feature space without explicitly transforming the data into that space.\n",
    "\n",
    "In summary, while polynomial functions and kernel functions are different types of functions, they are related in that polynomial functions can be used as kernel functions in machine learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f8b378-6c5d-4fe6-9a36-9e62fa3da238",
   "metadata": {},
   "source": [
    "**Q2. How can we implement an SVM with a polynomial kernel in Python using Scikit-learn?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca5a7f7-dce1-4a84-9770-61a4c33be45a",
   "metadata": {},
   "source": [
    "To implement a **support vector machine (SVM)** with a **polynomial kernel** in **Python** using **Scikit-learn**, you can follow these steps:\n",
    "\n",
    "**Step 1: Import the necessary libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75e71cb8-e43a-4a46-9c00-242750126ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e811f722-b401-4111-bfa7-7d9195308305",
   "metadata": {},
   "source": [
    "**Step 2: Generate a sample dataset for classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "155d03fe-7e33-4f46-90df-75d85cf248a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(n_samples=100, n_features=10, n_informative=5, n_classes=2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d475057-eee8-429e-86a2-3b55e5b4d56a",
   "metadata": {},
   "source": [
    "**Step 3: Split the data into training and testing sets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b23ed590-fbfe-4aa8-a782-2e3f492a9562",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7961a05-eebf-4348-beb3-06221b3e8b7c",
   "metadata": {},
   "source": [
    "**Step 4: Create an instance of the SVM model with the polynomial kernel**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "575ef0a4-2061-48aa-a8b5-37dd6b4a7293",
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_svm = SVC(kernel='poly', degree=3, gamma='scale')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e765499-1a55-42d6-9de3-b632769d4818",
   "metadata": {},
   "source": [
    "In the above code, we have specified the **kernel parameter** as **'poly'** to use the **polynomial kernel** and the **degree parameter as 3** to specify the **degree of the polynomial**.\n",
    "\n",
    "**Step 5: Fit the model to the training data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "525dcedb-9426-4905-85dd-f25902f280eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(kernel=&#x27;poly&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(kernel=&#x27;poly&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(kernel='poly')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poly_svm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566c4743-ce87-458f-9cd6-c03323650994",
   "metadata": {},
   "source": [
    "**Step 6: Make predictions on the test data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc7e5679-c594-4c03-bfe0-3894a4422b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = poly_svm.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c1a816-89be-40fd-afc1-d35269fd32d6",
   "metadata": {},
   "source": [
    "**Step 7: Evaluate the performance of the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a289be01-1314-410a-8533-7722484486e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.85\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb43fde3-dacc-4374-9a1f-bd107e5e81c6",
   "metadata": {},
   "source": [
    "In the above code, we have used the **accuracy_score function** from the **metrics module** of **Scikit-learn** to calculate the **accuracy of the model**.\n",
    "\n",
    "Overall, these steps show how you can implement a **support vector machine (SVM)** with a **polynomial kernel** in **Python** using **Scikit-learn**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16baf64b-351d-4fcf-9fe8-b9b33d45e059",
   "metadata": {},
   "source": [
    "**Q3. How does increasing the value of epsilon affect the number of support vectors in SVR?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7938c2e7-a54d-4872-9bf0-9821b87fb40c",
   "metadata": {},
   "source": [
    "In Support Vector Regression (SVR), the parameter epsilon is a hyperparameter that controls the trade-off between the flatness of the regression function and the degree to which errors are tolerated in the training data. Specifically, epsilon defines a margin of tolerance around the predicted value, within which errors are not penalized.\n",
    "\n",
    "Increasing the value of epsilon in SVR tends to increase the number of support vectors. This is because as the value of epsilon increases, the flatness of the regression function increases and the degree of error tolerance also increases. As a result, more data points are likely to fall within the margin of tolerance, which in turn leads to more support vectors.\n",
    "\n",
    "The number of support vectors in SVR has a direct impact on the complexity of the model and its ability to generalize to new data. In general, a higher number of support vectors can lead to longer training times and potentially overfitting to the training data. On the other hand, a lower number of support vectors can result in a simpler model that may not capture all of the complex relationships in the data.\n",
    "\n",
    "Therefore, the choice of epsilon should be carefully considered to strike a balance between model complexity and accuracy. In practice, the optimal value of epsilon is often determined through a process of cross-validation or grid search over a range of hyperparameter values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5f74c1-614c-4c6d-bbeb-f85ac4c03baa",
   "metadata": {},
   "source": [
    "**Q4. How does the choice of kernel function, C parameter, epsilon parameter, and gamma parameter affect the performance of Support Vector Regression (SVR)? Can you explain how each parameter works and provide examples of when you might want to increase or decrease its value?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e50945-e53a-4e87-bab4-de767a42a747",
   "metadata": {},
   "source": [
    "The **performance** of **Support Vector Regression (SVR)** is **heavily dependent** on the choice of **hyperparameters** such as the **kernel function, C parameter, epsilon parameter, and gamma parameter**. In this answer, we will discuss each **parameter** and explain **how it works** and when you might want **to increase or decrease its value**.\n",
    "1. **Kernel Function**: The choice of kernel function determines the mapping of the input data to a high-dimensional feature space. The kernel function is specified using the kernel hyperparameter in the SVR object. Scikit-learn provides several options for kernel functions, such as linear, polynomial, and radial basis function (RBF). The choice of kernel function depends on the nature of the data and the problem at hand. In general, the RBF kernel is a good default choice and works well in many scenarios.\n",
    "2. **C Parameter**: The C parameter controls the trade-off between model complexity and training error. It is specified using the C hyperparameter in the SVR object. A smaller value of C will result in a wider margin and a simpler model, while a larger value of C will result in a narrower margin and a more complex model. Increasing the value of C can lead to overfitting, while decreasing the value of C can lead to underfitting. You might want to increase the value of C if the model is underfitting, and decrease the value of C if the model is overfitting.\n",
    "3. **Epsilon Parameter**: The epsilon parameter determines the width of the margin around the predicted value within which errors are not penalized. It is specified using the epsilon hyperparameter in the SVR object. A larger value of epsilon will allow for more error in the model, while a smaller value of epsilon will make the model more sensitive to errors. You might want to increase the value of epsilon if the model is too sensitive to noise, and decrease the value of epsilon if the model is not sensitive enough to noise.\n",
    "4. **Gamma Parameter**: The gamma parameter determines the width of the Gaussian kernel for the RBF kernel function. It is specified using the gamma hyperparameter in the SVR object. A larger value of gamma will result in a narrower kernel and a more complex model, while a smaller value of gamma will result in a wider kernel and a simpler model. Increasing the value of gamma can lead to overfitting, while decreasing the value of gamma can lead to underfitting. You might want to increase the value of gamma if the model is underfitting, and decrease the value of gamma if the model is overfitting.\n",
    "\n",
    "In general, the choice of **hyperparameters** in **Support Vector Regression (SVR)** can have a **significant impact** on the **performance of the model**. It is **highly recommended** to test **different values** of these **hyperparameters** using **techniques** such as **cross-validation** and **grid search** to find the **optimal combination of hyperparameters** for the **given problem**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4a71aa-7e9d-4eb9-b468-7c2ed2502490",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b94072-9c04-497f-aec9-e69fd200fd26",
   "metadata": {},
   "source": [
    "**Q5. Assignment:**\n",
    "1. **Import the necessary libraries and load the dataset.**\n",
    "2. **Split the dataset into training and testing sets.**\n",
    "3. **Preprocess the data using any technique of your choice. (e.g. scaling, normalization)**\n",
    "4. **Create an instance of the SVC classifier and train it on the training data.**\n",
    "5. **Use the trained classifier to predict the labels of the testing data.**\n",
    "6. **Evaluate the performance of the classifier using any metric of your choice. (e.g. accuracy, precision, recall, F1-score)**\n",
    "7. **Tune the hyperparameters of the SVC classifier using GridSearchCV or RandomizedSearchCV to improve its performance.**\n",
    "8. **Train the tuned classifier on the entire dataset.**\n",
    "9. **Save the trained classifier to a file for future use.**\n",
    "\n",
    "**NOTE**: You can use any dataset of your choice for this assignment, but make sure it is suitable for classification and has a sufficient number of features and samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc176c6-f5dc-4f92-b4e2-9a62f8268c58",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26b8c01-382c-4b94-8040-363ff9b44c8c",
   "metadata": {},
   "source": [
    "**Here's an example of how to implement these steps using the Red Wine dataset from Seaborn library:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf85dd81-d789-4452-825c-f2a0568d7284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.60\n",
      "\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "[CV 1/5] END ...C=0.1, gamma=0.1, kernel=linear;, score=0.562 total time=   0.1s\n",
      "[CV 2/5] END ...C=0.1, gamma=0.1, kernel=linear;, score=0.555 total time=   0.1s\n",
      "[CV 3/5] END ...C=0.1, gamma=0.1, kernel=linear;, score=0.570 total time=   0.1s\n",
      "[CV 4/5] END ...C=0.1, gamma=0.1, kernel=linear;, score=0.613 total time=   0.1s\n",
      "[CV 5/5] END ...C=0.1, gamma=0.1, kernel=linear;, score=0.655 total time=   0.1s\n",
      "[CV 1/5] END ......C=0.1, gamma=0.1, kernel=rbf;, score=0.559 total time=   0.1s\n",
      "[CV 2/5] END ......C=0.1, gamma=0.1, kernel=rbf;, score=0.570 total time=   0.1s\n",
      "[CV 3/5] END ......C=0.1, gamma=0.1, kernel=rbf;, score=0.590 total time=   0.1s\n",
      "[CV 4/5] END ......C=0.1, gamma=0.1, kernel=rbf;, score=0.598 total time=   0.1s\n",
      "[CV 5/5] END ......C=0.1, gamma=0.1, kernel=rbf;, score=0.643 total time=   0.1s\n",
      "[CV 1/5] END .....C=0.1, gamma=1, kernel=linear;, score=0.562 total time=   0.1s\n",
      "[CV 2/5] END .....C=0.1, gamma=1, kernel=linear;, score=0.555 total time=   0.1s\n",
      "[CV 3/5] END .....C=0.1, gamma=1, kernel=linear;, score=0.570 total time=   0.1s\n",
      "[CV 4/5] END .....C=0.1, gamma=1, kernel=linear;, score=0.613 total time=   0.1s\n",
      "[CV 5/5] END .....C=0.1, gamma=1, kernel=linear;, score=0.655 total time=   0.1s\n",
      "[CV 1/5] END ........C=0.1, gamma=1, kernel=rbf;, score=0.430 total time=   0.1s\n",
      "[CV 2/5] END ........C=0.1, gamma=1, kernel=rbf;, score=0.434 total time=   0.1s\n",
      "[CV 3/5] END ........C=0.1, gamma=1, kernel=rbf;, score=0.430 total time=   0.2s\n",
      "[CV 4/5] END ........C=0.1, gamma=1, kernel=rbf;, score=0.430 total time=   0.2s\n",
      "[CV 5/5] END ........C=0.1, gamma=1, kernel=rbf;, score=0.431 total time=   0.1s\n",
      "[CV 1/5] END ....C=0.1, gamma=10, kernel=linear;, score=0.562 total time=   0.1s\n",
      "[CV 2/5] END ....C=0.1, gamma=10, kernel=linear;, score=0.555 total time=   0.1s\n",
      "[CV 3/5] END ....C=0.1, gamma=10, kernel=linear;, score=0.570 total time=   0.1s\n",
      "[CV 4/5] END ....C=0.1, gamma=10, kernel=linear;, score=0.613 total time=   0.1s\n",
      "[CV 5/5] END ....C=0.1, gamma=10, kernel=linear;, score=0.655 total time=   0.1s\n",
      "[CV 1/5] END .......C=0.1, gamma=10, kernel=rbf;, score=0.430 total time=   0.2s\n",
      "[CV 2/5] END .......C=0.1, gamma=10, kernel=rbf;, score=0.434 total time=   0.2s\n",
      "[CV 3/5] END .......C=0.1, gamma=10, kernel=rbf;, score=0.430 total time=   0.2s\n",
      "[CV 4/5] END .......C=0.1, gamma=10, kernel=rbf;, score=0.430 total time=   0.2s\n",
      "[CV 5/5] END .......C=0.1, gamma=10, kernel=rbf;, score=0.431 total time=   0.2s\n",
      "[CV 1/5] END .....C=1, gamma=0.1, kernel=linear;, score=0.562 total time=   0.1s\n",
      "[CV 2/5] END .....C=1, gamma=0.1, kernel=linear;, score=0.551 total time=   0.1s\n",
      "[CV 3/5] END .....C=1, gamma=0.1, kernel=linear;, score=0.559 total time=   0.1s\n",
      "[CV 4/5] END .....C=1, gamma=0.1, kernel=linear;, score=0.602 total time=   0.1s\n",
      "[CV 5/5] END .....C=1, gamma=0.1, kernel=linear;, score=0.647 total time=   0.1s\n",
      "[CV 1/5] END ........C=1, gamma=0.1, kernel=rbf;, score=0.605 total time=   0.1s\n",
      "[CV 2/5] END ........C=1, gamma=0.1, kernel=rbf;, score=0.586 total time=   0.1s\n",
      "[CV 3/5] END ........C=1, gamma=0.1, kernel=rbf;, score=0.609 total time=   0.1s\n",
      "[CV 4/5] END ........C=1, gamma=0.1, kernel=rbf;, score=0.641 total time=   0.1s\n",
      "[CV 5/5] END ........C=1, gamma=0.1, kernel=rbf;, score=0.686 total time=   0.1s\n",
      "[CV 1/5] END .......C=1, gamma=1, kernel=linear;, score=0.562 total time=   0.1s\n",
      "[CV 2/5] END .......C=1, gamma=1, kernel=linear;, score=0.551 total time=   0.1s\n",
      "[CV 3/5] END .......C=1, gamma=1, kernel=linear;, score=0.559 total time=   0.1s\n",
      "[CV 4/5] END .......C=1, gamma=1, kernel=linear;, score=0.602 total time=   0.1s\n",
      "[CV 5/5] END .......C=1, gamma=1, kernel=linear;, score=0.647 total time=   0.1s\n",
      "[CV 1/5] END ..........C=1, gamma=1, kernel=rbf;, score=0.645 total time=   0.2s\n",
      "[CV 2/5] END ..........C=1, gamma=1, kernel=rbf;, score=0.648 total time=   0.2s\n",
      "[CV 3/5] END ..........C=1, gamma=1, kernel=rbf;, score=0.621 total time=   0.2s\n",
      "[CV 4/5] END ..........C=1, gamma=1, kernel=rbf;, score=0.668 total time=   0.2s\n",
      "[CV 5/5] END ..........C=1, gamma=1, kernel=rbf;, score=0.682 total time=   0.2s\n",
      "[CV 1/5] END ......C=1, gamma=10, kernel=linear;, score=0.562 total time=   0.1s\n",
      "[CV 2/5] END ......C=1, gamma=10, kernel=linear;, score=0.551 total time=   0.1s\n",
      "[CV 3/5] END ......C=1, gamma=10, kernel=linear;, score=0.559 total time=   0.1s\n",
      "[CV 4/5] END ......C=1, gamma=10, kernel=linear;, score=0.602 total time=   0.1s\n",
      "[CV 5/5] END ......C=1, gamma=10, kernel=linear;, score=0.647 total time=   0.1s\n",
      "[CV 1/5] END .........C=1, gamma=10, kernel=rbf;, score=0.562 total time=   0.2s\n",
      "[CV 2/5] END .........C=1, gamma=10, kernel=rbf;, score=0.562 total time=   0.2s\n",
      "[CV 3/5] END .........C=1, gamma=10, kernel=rbf;, score=0.547 total time=   0.2s\n",
      "[CV 4/5] END .........C=1, gamma=10, kernel=rbf;, score=0.578 total time=   0.2s\n",
      "[CV 5/5] END .........C=1, gamma=10, kernel=rbf;, score=0.569 total time=   0.2s\n",
      "[CV 1/5] END ....C=10, gamma=0.1, kernel=linear;, score=0.559 total time=   0.2s\n",
      "[CV 2/5] END ....C=10, gamma=0.1, kernel=linear;, score=0.543 total time=   0.2s\n",
      "[CV 3/5] END ....C=10, gamma=0.1, kernel=linear;, score=0.559 total time=   0.2s\n",
      "[CV 4/5] END ....C=10, gamma=0.1, kernel=linear;, score=0.598 total time=   0.2s\n",
      "[CV 5/5] END ....C=10, gamma=0.1, kernel=linear;, score=0.647 total time=   0.2s\n",
      "[CV 1/5] END .......C=10, gamma=0.1, kernel=rbf;, score=0.629 total time=   0.1s\n",
      "[CV 2/5] END .......C=10, gamma=0.1, kernel=rbf;, score=0.609 total time=   0.1s\n",
      "[CV 3/5] END .......C=10, gamma=0.1, kernel=rbf;, score=0.594 total time=   0.1s\n",
      "[CV 4/5] END .......C=10, gamma=0.1, kernel=rbf;, score=0.656 total time=   0.1s\n",
      "[CV 5/5] END .......C=10, gamma=0.1, kernel=rbf;, score=0.678 total time=   0.1s\n",
      "[CV 1/5] END ......C=10, gamma=1, kernel=linear;, score=0.559 total time=   0.2s\n",
      "[CV 2/5] END ......C=10, gamma=1, kernel=linear;, score=0.543 total time=   0.2s\n",
      "[CV 3/5] END ......C=10, gamma=1, kernel=linear;, score=0.559 total time=   0.2s\n",
      "[CV 4/5] END ......C=10, gamma=1, kernel=linear;, score=0.598 total time=   0.2s\n",
      "[CV 5/5] END ......C=10, gamma=1, kernel=linear;, score=0.647 total time=   0.2s\n",
      "[CV 1/5] END .........C=10, gamma=1, kernel=rbf;, score=0.590 total time=   0.2s\n",
      "[CV 2/5] END .........C=10, gamma=1, kernel=rbf;, score=0.648 total time=   0.2s\n",
      "[CV 3/5] END .........C=10, gamma=1, kernel=rbf;, score=0.633 total time=   0.2s\n",
      "[CV 4/5] END .........C=10, gamma=1, kernel=rbf;, score=0.641 total time=   0.2s\n",
      "[CV 5/5] END .........C=10, gamma=1, kernel=rbf;, score=0.671 total time=   0.2s\n",
      "[CV 1/5] END .....C=10, gamma=10, kernel=linear;, score=0.559 total time=   0.2s\n",
      "[CV 2/5] END .....C=10, gamma=10, kernel=linear;, score=0.543 total time=   0.2s\n",
      "[CV 3/5] END .....C=10, gamma=10, kernel=linear;, score=0.559 total time=   0.2s\n",
      "[CV 4/5] END .....C=10, gamma=10, kernel=linear;, score=0.598 total time=   0.2s\n",
      "[CV 5/5] END .....C=10, gamma=10, kernel=linear;, score=0.647 total time=   0.2s\n",
      "[CV 1/5] END ........C=10, gamma=10, kernel=rbf;, score=0.562 total time=   0.2s\n",
      "[CV 2/5] END ........C=10, gamma=10, kernel=rbf;, score=0.570 total time=   0.2s\n",
      "[CV 3/5] END ........C=10, gamma=10, kernel=rbf;, score=0.547 total time=   0.2s\n",
      "[CV 4/5] END ........C=10, gamma=10, kernel=rbf;, score=0.574 total time=   0.2s\n",
      "[CV 5/5] END ........C=10, gamma=10, kernel=rbf;, score=0.569 total time=   0.2s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['svc_model.joblib']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Import the necessary libraries and load the dataset\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import joblib\n",
    "df = pd.read_csv('winequality-red.csv')\n",
    "\n",
    "# 2. Split the dataset into training and testing sets\n",
    "X = df.drop('quality', axis=1)\n",
    "y = df['quality']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 3. Preprocess the data using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 4. Create an instance of the SVC classifier and train it on the training data\n",
    "svc = SVC()\n",
    "svc.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 5. Use the trained classifier to predict the labels of the testing data\n",
    "y_pred = svc.predict(X_test_scaled)\n",
    "\n",
    "# 6. Evaluate the performance of the classifier using accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"\")\n",
    "\n",
    "# 7. Tune the hyperparameters of the SVC classifier using GridSearchCV\n",
    "param_grid = {'C': [0.1, 1, 10], 'gamma': [0.1, 1, 10], 'kernel': ['linear', 'rbf']}\n",
    "grid = GridSearchCV(SVC(), param_grid, refit=True, verbose=3)\n",
    "grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 8. Train the tuned classifier on the entire dataset\n",
    "best_svc = grid.best_estimator_\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "best_svc.fit(X_scaled, y)\n",
    "\n",
    "# 9. Save the trained classifier to a file for future use\n",
    "joblib.dump(best_svc, 'svc_model.joblib')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
