{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6136dea7-4dc2-49b7-98e1-92f319896602",
   "metadata": {},
   "source": [
    "**Q1. What is Bayes' theorem?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58766f5f-f471-4253-b685-26bccfe0d578",
   "metadata": {},
   "source": [
    "Bayes' theorem, named after Reverend Thomas Bayes, is a mathematical formula that describes the relationship between conditional probabilities.\n",
    "\n",
    "In its simplest form, Bayes' theorem states that the probability of a hypothesis H given some observed evidence E is proportional to the probability of the evidence E given the hypothesis H, multiplied by the prior probability of the hypothesis H: **P(H|E) = P(E|H) * P(H) / P(E)**\n",
    "\n",
    "Where:<br>\n",
    "P(H|E) is the posterior probability of H given E (what we want to know)<br>P(E|H) is the likelihood of observing the evidence E given the hypothesis H (how well the evidence supports the hypothesis)<br>P(H) is the prior probability of the hypothesis H (our initial belief in the hypothesis before seeing the evidence)<br>P(E) is the marginal probability of the evidence E (the total probability of observing the evidence, regardless of the hypothesis)\n",
    "\n",
    "Bayes' theorem is a fundamental concept in probability & has many applications in fields such as statistics, machine learning, and artificial intelligence. It provides a way to update our beliefs about the world in light of new evidence and is essential for making informed decisions in uncertain situations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b5579a-8497-45f3-851a-a240e28c685f",
   "metadata": {},
   "source": [
    "**Q2. What is the formula for Bayes' theorem?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9cb4ecb-44e6-4568-8f66-6d6002c40585",
   "metadata": {},
   "source": [
    "The formula for Bayes' theorem is:<br>**P(H|E) = P(E|H) * P(H) / P(E)**\n",
    "\n",
    "Where:<br>\n",
    "P(H|E) is the posterior probability of H given E (what we want to know)<br>P(E|H) is the likelihood of observing the evidence E given the hypothesis H (how well the evidence supports the hypothesis)<br>P(H) is the prior probability of the hypothesis H (our initial belief in the hypothesis before seeing the evidence)<br>P(E) is the marginal probability of the evidence E (the total probability of observing the evidence, regardless of the hypothesis)\n",
    "\n",
    "This formula is used to update our beliefs about the probability of a hypothesis H being true based on new evidence E. It is a powerful tool in Bayesian statistics and machine learning, allowing us to make more accurate predictions and decisions based on data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b289975-01b8-4f73-900b-56b55beb0d6f",
   "metadata": {},
   "source": [
    "**Q3. How is Bayes' theorem used in practice?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b233ed6-b291-4229-9e85-99f2eff7c202",
   "metadata": {},
   "source": [
    "Bayes' theorem is used in practice in a wide range of fields, including statistics, machine learning, artificial intelligence, and decision-making. Also, Bayes' theorem is used in some other applications, which are given below:\n",
    "1. **Medical diagnosis**: In medicine, Bayes' theorem can be used to calculate the probability of a patient having a disease given their symptoms and other medical information. Doctors can use this information to make more informed decisions about diagnosis and treatment.\n",
    "2. **Spam filtering**: Bayes' theorem is used in spam filtering algorithms to calculate the probability that an email is spam given its contents. This helps email providers identify and filter out unwanted messages.\n",
    "3. **Predictive modeling**: In machine learning, Bayes' theorem is used to build predictive models that can make accurate predictions based on past data. This can be useful in a wide range of applications, from stock market forecasting to weather prediction.\n",
    "4. **A/B testing**: Bayes' theorem is used in A/B testing to determine the probability that one version of a product or service is better than another based on user behavior. This helps businesses optimize their offerings and improve customer satisfaction.\n",
    "5. **Decision-making**: Bayes' theorem can be used to make decisions in uncertain situations by calculating the expected value of different options. This approach is often used in game theory, finance, and other fields where decision-making under uncertainty is important.\n",
    "\n",
    "Overall, Bayes' theorem provides a powerful framework for making predictions and decisions based on data and has numerous practical applications in a wide range of fields."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce27e3a8-cb67-484c-af7a-6dea4b002aca",
   "metadata": {},
   "source": [
    "**Q4. What is the relationship between Bayes' theorem and conditional probability?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da61ac6-69c1-4629-bb8c-55b3006a7f06",
   "metadata": {},
   "source": [
    "Bayes' theorem is a mathematical formula that describes the relationship between conditional probabilities. In fact, Bayes' theorem can be derived from the definition of conditional probability.\n",
    "\n",
    "Conditional probability is the probability of an event states that another event has already occurred. For example, the probability of rolling a six on a die given that the die has already been rolled is a conditional probability. Denoted as P(A|B), where A and B are events, it's stated as \"Probability of A given B.\"\n",
    "\n",
    "Bayes' theorem provides a way to calculate the conditional probability of one event given another event and some prior information. It allows us to update our prior beliefs about the probability of an event occurring based on new evidence.\n",
    "\n",
    "The formula for Bayes' theorem involves multiplying the likelihood of the observed evidence given the hypothesis by the prior probability of the hypothesis and dividing by the marginal probability of the evidence. In this way, Bayes' theorem combines prior beliefs with new evidence to calculate a revised probability of a hypothesis being true.\n",
    "\n",
    "So, in summary, Bayes' theorem and conditional probability are related because Bayes' theorem provides a way to calculate conditional probabilities based on prior information and new evidence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db07c65-f2cd-4a43-9c0c-7a3a889b1e4e",
   "metadata": {},
   "source": [
    "**Q5. How do you choose which type of Naive Bayes classifier to use for any given problem?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa7c01d-6cc2-4541-9795-d3bc3bf03062",
   "metadata": {},
   "source": [
    "The Naive Bayes classifier is a simple and effective machine learning algorithm that is used for a wide range of applications such as text classification, spam filtering, sentiment analysis, and more. There are three types of Naive Bayes classifiers: Gaussian Naive Bayes, Multinomial Naive Bayes, and Bernoulli Naive Bayes. The choice of which Naive Bayes classifier to use for a given problem depends on the nature of the data and the problem at hand. Here are some guidelines to help you choose the appropriate type of Naive Bayes classifier:\n",
    "1. **Gaussian Naive Bayes**: This type of classifier is used when the features are continuous variables that follow a Gaussian distribution. It is often used for problems such as image classification and spam filtering, where the features are numeric values.\n",
    "2. **Multinomial Naive Bayes**: This type of classifier is used when the features are discrete variables such as word counts in text classification problems. It is often used in natural language processing applications, such as sentiment analysis and text classification.\n",
    "3. **Bernoulli Naive Bayes**: This type of classifier is similar to the Multinomial Naive Bayes classifier, but it is used for solving some text classification problems when the features are binary variables such as the presence or absence of a particular word in a document.\n",
    "\n",
    "In summary, the choice of Naive Bayes classifier depends on the nature of the data and the problem at hand. Gaussian Naive Bayes is used for continuous variables, Multinomial Naive Bayes for discrete variables, and Bernoulli Naive Bayes for binary variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9574295-c895-478d-a09e-387a4c4482cf",
   "metadata": {},
   "source": [
    "**Q6. Assignment:**<br>\n",
    "**You have a dataset with two features, X1 and X2, and two possible classes, A and B. You want to use Naive Bayes to classify a new instance with features X1 = 3 and X2 = 4. The following table shows the frequency of each feature value for each class:**<br>\n",
    "**A=[3, 3, 4, 4, 3, 3, 3]**<br>\n",
    "**B=[2, 2, 1, 2, 2, 2, 3]**<br>\n",
    "**Assuming equal prior probabilities for each class, which class would Naive Bayes predict the new instance to belong to?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adaeaa24-ee7f-44b3-bd3c-fd0d04134cac",
   "metadata": {},
   "source": [
    "To use Naive Bayes to classify the new instance with features X1 = 3 and X2 = 4, we need to calculate the posterior probability of the instance belonging to each class, given the values of X1 and X2. The formula for Naive Bayes is given below:\n",
    "\n",
    "**P(class|X1,X2) = P(X1,X2|class) * P(class) / P(X1,X2)**\n",
    "\n",
    "where class is the class label A or B, and P(X1,X2|class) denotes observing the feature values given the class label. P(class) is the prior probability of the class, and P(X1,X2) is the marginal probability of observing the feature values.\n",
    "\n",
    "To calculate the likelihood of observing the feature values for each class, we can use the frequency table provided in the problem:\n",
    "\n",
    "P(X1=3,X2=4|A) = 3/7 * 4/7 = 0.1224<br>\n",
    "P(X1=3,X2=4|B) = 0/7 * 1/7 = 0\n",
    "\n",
    "To calculate the prior probability of each class, we assume that they are equal, i.e., P(A) = P(B) = 0.5.\n",
    "\n",
    "To calculate the marginal probability of observing the feature values, we can use the law of total probability:\n",
    "\n",
    "P(X1=3,X2=4) = P(X1=3,X2=4|A) * P(A) + P(X1=3,X2=4|B) * P(B)<br>\n",
    "= 0.1224 * 0.5 + 0 * 0.5<br>\n",
    "= 0.0612\n",
    "\n",
    "Finally, we can calculate the posterior probability of the instance belonging to each class:\n",
    "\n",
    "P(A|X1=3,X2=4) = P(X1=3,X2=4|A) * P(A) / P(X1=3,X2=4)<br>\n",
    "= 0.1224 * 0.5 / 0.0612<br>\n",
    "= 1\n",
    "\n",
    "P(B|X1=3,X2=4) = P(X1=3,X2=4|B) * P(B) / P(X1=3,X2=4)<br>\n",
    "= 0 * 0.5 / 0.0612<br>\n",
    "= 0\n",
    "\n",
    "Therefore, Naive Bayes would predict that the new instance belongs to class A.\n",
    "\n",
    "**ALTERNATIVE METHOD**\n",
    "\n",
    "To predict the class of a new instance with features X1 = 3 and X2 = 4 using Naive Bayes, we need to calculate the posterior probabilities of the instance belonging to each class, given the observed feature values. We can use the Naive Bayes formula:\n",
    "\n",
    "**P(class|X1=3,X2=4) = P(X1=3,X2=4|class) * P(class) / P(X1=3,X2=4)**\n",
    "\n",
    "where P(X1=3,X2=4|class) is the likelihood of observing the feature values given the class, P(class) is the prior probability of the class, and P(X1=3,X2=4) is the marginal probability of observing the feature values.\n",
    "\n",
    "To calculate the likelihood of observing the feature values for each class, we can count the number of times each feature value occurs in the training data for each class and compute the corresponding probabilities:\n",
    "\n",
    "P(X1=3|A) = 4/7, P(X2=4|A) = 3/7, P(X1=3|B) = 0/7, P(X2=4|B) = 1/7\n",
    "\n",
    "Using these values, we can calculate the posterior probabilities of the new instance belonging to each class, given the observed feature values:\n",
    "\n",
    "P(A|X1=3,X2=4) = P(X1=3,X2=4|A) * P(A) / P(X1=3,X2=4) = (4/7) * (1/2) / ((4/7)(1/2) + (0/7)(1/2)) = 1<br>\n",
    "P(B|X1=3,X2=4) = P(X1=3,X2=4|B) * P(B) / P(X1=3,X2=4) = (0/7) * (1/2) / ((4/7)(1/2) + (0/7)(1/2)) = 0\n",
    "\n",
    "Therefore, Naive Bayes would predict that the new instance belongs to class A."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
